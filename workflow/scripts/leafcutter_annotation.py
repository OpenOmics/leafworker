#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Author: Skyler Kuhn

# Standard Library
from __future__ import print_function
from datetime import datetime
from textwrap import dedent
import argparse, gzip, os, sys

# Constants
# Usage and help section 
_HELP = dedent("""
@Usage:
    $ ./leafcutter_annotation.py [-h] [--version] \\
            [--fdr-filter FDR_FILTER] \\
            --effect-sizes EFFECT_SIZES_FILE \\
            --cluster-signif CLUSTER_SIGNIF_FILE \\
            --intron-ann INTRON_ANN_FILE \\
            --splicing-ann SPLICING_ANN_FILE \\
            --output OUTPUT_FILE
@About:
    Given the differential splicing results from
    leafcutter (effect size, cluster signif output
    files), an intron annotation file, and a splicing
    annotation file, this script will collate info
    from each source to add the cluster adjusted
    p-values, transcript information and exon
    information to the intron effect sizes file.
    This allow a user to quickly filter the output
    from leafcutter and to see what transcripts and
    exons are associated with any characterized
    differential splicing events.

    The results can also be filtered based on an
    adjusted p-value threshold, where the default
    is set to "0.1".

@Required:
    -e, --effect-sizes EFFECT_SIZES_FILE
        Input leafcutter effect sizes file.
        This file is generated by running
        "leafcutter_ds.R".
    -c, --cluster-signif CLUSTER_SIGNIF_FILE
        Input leafcutter cluster significance
        file. This file is generated by
        running "leafcutter_ds.R".
    -i, --intron-ann INTRON_ANN_FILE
        Input intron annotation file. This
        file was generated by exporting the
        "intron" table from the Rdata file
        generated by "prepare_results.R".
    -s, --splicing-ann SPLICING_ANN_FILE
        Input splicing annotation file. This 
        file was generated by parsing exon
        and transcript information from the
        GTF file. It is the output file of
        "splicing_annotation.py".
    -o, --output OUTPUT_FILE
        Output file with merged and annotated
        leafcutter results.
@Options:
    -f, --fdr-filter FDR_FILTER
        Adjusted p-value filter. This option
        will filter the results to only focus
        on differential splicing events with
        a cluster significance less than or
        equal to this value, default: "0.1".
    -h, --help
        Shows help message and exits.
    -v, --version
        Prints the version and exits.

@Example:
    $ ./leafcutter_annotation.py \\
        -e leafcutter_effect_sizes.txt \\
        -c leafcutter_cluster_significance.txt \\
        -i intron_annotation.tsv \\
        -s splicing_annotation.tsv \\
        -o leafcutter_annotated_results.tsv \\
        -f 0.1
"""
)

# Semantic version
_VERISON = '1.0.0'


# Helper functions
def err(*message, **kwargs):
    """Prints any provided args to standard error.
    kwargs can be provided to modify print functions
    behavior.
    @param message <any>:
        Values printed to standard error
    @params kwargs <print()>
        Key words to modify print function behavior
    """
    print(*message, file=sys.stderr, **kwargs)


def fatal(*message, **kwargs):
    """Prints any provided args to standard error
    and exits with an exit code of 1.
    @param message <any>:
        Values printed to standard error
    @params kwargs <print()>
        Key words to modify print function behavior
    """
    err(*message, **kwargs)
    sys.exit(1)


def timestamp(format="%Y-%m-%d %H:%M:%S"):
    """Returns a formatted timestamp string
    for the current time.
    @param format <str>:
        Format string for the timestamp, default:
        "%Y-%m-%d %H:%M:%S" which is equivalent to
        "2023-10-01 12:00:00" for example.
    @return <str>:
        Formatted timestamp string, i.e. "2023-10-01 12:00:00"
    """
    return datetime.now().strftime(format)


def log(*message):
    """Logs a message to standard output with a timestamp.
    @param message <any>:
        Values printed to log
    """
    print("[{0}] {1}".format(
        timestamp(),
        " ".join([str(m) for m in message]))
    )


def check_permissions(parser, path, *args, **kwargs):
    """Checks permissions using os.access() to see the
    user is authorized to access a file/directory. Checks
    for existence, read, write and execute via args:
        • os.F_OK (tests existence)
        • os.R_OK (tests read)
        • os.W_OK (tests write)
        • os.X_OK (tests exec)
    @param parser <argparse.ArgumentParser() object>:
        Argparse parser object
    @param path <str>:
        Name of path to check
    @param args <any>:
        Positional args to pass to os.access()
    @param kwargs <any>:
        Named kwargs to pass to os.access()
    @return path <str>:
        Returns absolute path if it exists and the
        checked permssions are setup are correct.
    """
    if not os.path.exists(path):
        parser.error(
            "Path '{}' does not exists! Failed to provide vaild input.".format(path)
        )
    if not os.access(path, *args, **kwargs):
        parser.error(
            "Path '{}' exists, but cannot read path due to permissions!".format(path)
        )
    return os.path.abspath(path)


def parse_cli_arguments():
    """Parses command line arguments and returns
    an argparse.parse_args object.
    @return <argparse.parse_args()>:
        Parsed command line arguments
    """
    parser = argparse.ArgumentParser(
        add_help=False,
        description=_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        usage = argparse.SUPPRESS,
    )
    # Leafcutter effect sizes file
    parser.add_argument(
        '-e', '--effect-sizes',
        type = lambda file: \
            check_permissions(parser, file, os.R_OK),
        required=True,
        help=argparse.SUPPRESS
    )
    # Leafcutter cluster signif file
    parser.add_argument(
        '-c', '--cluster-signif',
        type = lambda file: \
            check_permissions(parser, file, os.R_OK),
        required=True,
        help=argparse.SUPPRESS
    )
    # Intron annotation file
    parser.add_argument(
        '-i', '--intron-ann',
        type = lambda file: \
            check_permissions(parser, file, os.R_OK),
        required=True,
        help=argparse.SUPPRESS
    )
    # Splicing annotation file
    parser.add_argument(
        '-s', '--splicing-ann',
        type = lambda file: \
            check_permissions(parser, file, os.R_OK),
        required=True,
        help=argparse.SUPPRESS
    )
    # Annotated output results
    parser.add_argument(
        '-o', '--output',
        type = str,
        required=True,
        help=argparse.SUPPRESS
    )
    # FDR filtering threshold
    parser.add_argument(
        '-f', '--fdr-filter',
        type=float, required=False,
        help=argparse.SUPPRESS,
        default=0.1
    )
    # Get version information
    parser.add_argument(
        '-v', '--version',
        action='version',
        help = argparse.SUPPRESS,
        version='%(prog)s {0}'.format(_VERISON)
    )
    # Add custom help message
    parser.add_argument(
        '-h', '--help',
        action='help',
        help=argparse.SUPPRESS
    )
    return parser.parse_args()


def stripped(s):
    """Cleans string to remove quotes
    @param s <str>:
        String to remove quotes or clean
    @return s <str>:
        Cleaned string with quotes removed
    """
    return s.strip('"').strip("'")


def index_header(file_header):
    """Returns the index of each column_name
    as a dictionary.
    @param file_header <str>:
        First line of a file, containing column names
    @return idx <dict[str]=int>:
        Column name to index dictionary
    """
    idx = {}
    tokens = [
        stripped(c.strip()) \
            for c in file_header.strip().split('\t')
    ]
    # Create column name to index mapping
    for i,c in enumerate(tokens):
        idx[c]=i
    return idx


def index_file(file, keys, key_delim, values):
    """Parses and indexes a file into a dictionary for quick
    lookups later. The file will be indexed on the column names
    of the keys provided and the values will be stored as a
    nested dictionary. If multiple keys are provided, they
    are concatnated into a single string where key_delim
    is the delimeter.
    @param file <str>:
        File to parse and index. Must contain a header with
        the columns listed in keys and values. The index of
        these columns will be automatically resolved.  
    @param keys <list[str]>:
        List of column names to index the file on. If more
        than one key is provide, then a index will be created
        by concatenating the keys into a single string where
        key_delim is the delimeter.
    @param values <list[str]>:
        List of column names to associate with each key. These
        values will be stored as a nest dictionary so they can
        be pulled by their name.   
    @return file_idx <dict[str]=str>:
        Nested dictionary where,
            • key = 'key_delim'.join(keys)
            • value = {val_col1: "A", val_col2:"B"}
        Given,
            keys=["A","B"], values["C","D"], key_delim="|"
            returns {"A|B": {"C": "c_i", "D": "d_i"}}
    """
    log("Started indexing input file: ", file)
    file_idx = {}
    # Handler for opening files, i.e.
    # uncompressed or gzip files
    open_func = gzip.open if file.endswith('.gz') else open
    line_number = 0  # Used for error reporting 
    with open_func(file, 'rt') as fh:
        header = next(fh)
        col_idx = index_header(header)
        for line in fh:
            # Split the line into columns
            tokens = line.strip().split('\t')
            # Concatente mutiple keys into
            # a single key separated by the
            # key_delim character
            _k = key_delim.join([tokens[col_idx[k]] for k in keys])
            _v = {v: tokens[col_idx[v]]  for v in values}
            file_idx[_k] = _v
    log("Completed indexing input file: ", file)
    return file_idx 


def parse_intron(line_list, intron_idx):
    """Parses intron information into tokens from a line list
    in the effect sizes file.
    @param line_list <list[str]>:
        List of tokens from a line in the effect sizes file,
        where each token represents a column in the file.
    @param intron_idx <int>:
        Index of the intron column in the line_list.
        The intron column is formatted as:
        "{chr}:{intron_start}:{intron_end}:{clust_id}"
    @return <tuple>:
        Returns a tuple of the form:
        (chromosome, intron_start, intron_end, cluster_id)
        where each element is a string.
        If the intron format is invalid, it returns None.
    """
    intron = line_list[intron_idx]
    try:
        chrom, start, stop, cluster_id = intron.split(":")
    except ValueError:
        err("Warning: Invalid intron format in provided effect sizes file!")
        err(" └── Expected format = chr:intron_start:intron_end:clust_id")
        err(" └── Encountered format = {0}".format(intron))
        return None
    return (chrom, start, stop, cluster_id)


def index_annotated_splice_junctions(
        file, 
        single_value_features=["transcript_name", "exon_strand"],
        multi_value_features=["exon_id", "exon_number", "exon_start_end"],
        multi_value_delim="|"
    ):
    """Indexes the annotated splice junctions from the
    splicing annotation file passed via the --splicing-ann
    command line argument. This function returns a dictionary
    containing exon-exon pair information for a given
    transcript for a given splice junction.
    Where:
                           SpliceJunction
                           |            |
    transcriptX ---#########------------######-------
                       exonA            exonB
    @param file <str>:
        Splicing annotation file containing exon
        information for each transcript.
    @param single_value_features <list[str]>:
        List of features that are single valued
        for each splice junction. These features
        will be stored as a single value in the
        output dictionary.
    @param multi_value_features <list[str]>:
        List of features that are multi valued
        for each splice junction. These features
        reprsent surrounding exon information
        and will be stored as a single value
        in the output dictionary, where each
        value is delimited by multi_value_delim.
    @param multi_value_delim <str>:
        Delimiter used to separate multiple values
        for multi valued features in the output
        dictionary. Default is "|".
    @return <dict[str]=dict[str]>:
        Returns a dictionary where:
        key = "{transcript}:{exonA_end}:{exonB_start}"
        value = Dictionary containing additional
        information about the exons surrounding
        the splice junction. This dictionary has
        the following keys:
            • transcript_name: The name of the
                transcript the splice junction is
                associated with.
            • exon_strand: Strand the exon is on.
            • exon_id: The identifiers of the
                surrounding exons. Information for
                ExonA and ExonB are delimited by
                a pipe "|" character.
            • exon_number: The number of the
                surrounding exons. Information for
                ExonA and ExonB are delimited by
                a pipe "|" character.
            • exon_start_end: Start and end position
                of the surrounding exons. Information
                for ExonA and ExonB are delimited by
                a pipe "|" character.
    """
    log("Started indexing input file: ", file)
    file_idx = {}
    # Handler for opening files, i.e.
    # uncompressed or gzip files
    open_func = gzip.open if file.endswith('.gz') else open
    line_number = 0  # Used for error reporting
    with open_func(file, 'rt') as fh:
        header = next(fh)
        col_idx = index_header(header)
        for line in fh:
            # Split the line into columns
            tokens = line.strip().split('\t')
            # Parse the splice junction information
            transcript = tokens[col_idx["transcript_id"]]
            nexons = len(tokens[col_idx["exon_id"]].split(multi_value_delim))
            for i in range(0, nexons-1):
                exonA_end = tokens[col_idx["exon_start_end"]].split(
                    multi_value_delim
                )[i].split(":")[1]
                exonB_start = tokens[col_idx["exon_start_end"]].split(
                    multi_value_delim
                )[i+1].split(":")[0]
                # Create the key for the splice junction
                _k = "{0}:{1}:{2}".format(
                    transcript, exonA_end, exonB_start
                )
                if _k not in file_idx:
                    # Initialize the dictionary for this key
                    file_idx[_k] = {}
                # Parse single and multi valued features
                for feature in single_value_features:
                    file_idx[_k][feature] = tokens[col_idx[feature]]
                for feature in multi_value_features:
                    # Join multi valued features with the
                    # multi_value_delim character
                    file_idx[_k][feature] = multi_value_delim.join(
                        tokens[col_idx[feature]].split(multi_value_delim)[i:i+2]
                    )
    log("Completed indexing input file: ", file)
    return file_idx


def get_additional_annotation_information(annotation_dict, first_key, values):
    """Get additional annotation information from a nested 
    dictionary using the first_key and each value in values
    as a composite key. Returns a list of values corresponding
    to the provided (first_key, value) pairs in annotation_dict.
    If a key is not found, it returns "NA" for that value.
    @param annotation_dict <dict>:
        Nested dictionary containing additional annotation information
        keyed by [first_key][v] where v is an element in values.
    @param first_key <str>:
        First key in the nested dictionary to use for lookups.
    @param values <list[str]>:
        List of values to retrieve from the dictionary. This is
        the second key in the nested dictionary.
    @return <list[str]>:
        Returns a list of values corresponding to the provided keys.
        If a key is not found, it returns "NA" for that value.
    """
    return [annotation_dict.get(first_key, {}).get(v, "NA") for v in values]


if __name__ == '__main__':
    # Parse command line arguments
    args = parse_cli_arguments()
    
    # Sanity check for usage
    if len(sys.argv) == 1:
        # Nothing was provided
        fatal('Invalid usage: {0} [-h] ...'.format(os.path.basename(sys.argv[0])))
    
    log("Running leafcutter annotation script with the following options: ", args)
    # Create output directory if
    # it does not exist
    output_dir = os.path.abspath(os.path.dirname(args.output))
    if not os.path.exists(output_dir):
        try: os.makedirs(output_dir)
        except OSError as e: 
            fatal(
                "Fatal error: Failed to create output directory: {0}\n{1}".format(
                    output_dir, e
                )
            )
    
    # Parse cluster_id and adjusted pvalues,
    # from leafcutter output file where:
    #   key = {chr}:{clust_id}
    ADJ_P_COLUMN_NAME = "p.adjust" 
    PARSE_CLUSTER_SIGNIF = ["df", ADJ_P_COLUMN_NAME]
    ADJ_P_COLUMN_IDX = PARSE_CLUSTER_SIGNIF.index(ADJ_P_COLUMN_NAME)
    cluster_signif_dict = index_file(
        args.cluster_signif,
        keys=["cluster"],
        values=PARSE_CLUSTER_SIGNIF,
        key_delim=""
    )

    # Parse gene, ensembl_id, verdict,
    # and transcripts from leafviz intron
    # annotation file where:
    #   key = {chr}:{intron_start}:{intron_end}:{clust_id}
    PARSE_INTRON_ANN = ["gene","ensemblID","verdict","transcripts"]
    INTRON_TRANSCRIPT_IDX = PARSE_INTRON_ANN.index("transcripts")
    INTRON_VERDICT_IDX = PARSE_INTRON_ANN.index("verdict")
    intron_ann_dict = index_file(
        args.intron_ann,
        keys=["chr","start","end","clusterID"],
        values=PARSE_INTRON_ANN,
        key_delim=":"
    )

    # Parse exon information from the
    # splicing annotation file where:
    #   key = {transcript}:{exonA_end}:{exonB_start}
    SINGULAR_EXON_FEATURES = ["transcript_name", "exon_strand"]
    SURRONDING_EXON_FEATURES = ["exon_id", "exon_number", "exon_start_end"]
    splicing_ann_dict = index_annotated_splice_junctions(
        args.splicing_ann,
        single_value_features=SINGULAR_EXON_FEATURES,
        multi_value_features=SURRONDING_EXON_FEATURES,
        multi_value_delim="|"
    )

    # Loop through effect sizes file
    # and add more detailed information
    log("Started writing annotated output file: ", args.output)
    ofh = open(args.output, "w")
    with open(args.effect_sizes, "r") as ifh:
        input_header = next(ifh).rstrip().split("\t") + PARSE_CLUSTER_SIGNIF + PARSE_INTRON_ANN + SINGULAR_EXON_FEATURES + SURRONDING_EXON_FEATURES
        output_header = "\t".join(input_header)
        intron_idx = input_header.index("intron")
        ofh.write(output_header + "\n")
        for line in ifh:
            # Split the line into columns
            tokens = line.rstrip().split('\t')
            # Parse intron info, where intron column format:
            # {chr}:{intron_start}:{intron_end}:{clust_id}
            intron = parse_intron(tokens, intron_idx)  # returns (chr,intron_start,intron_end,clust_id)
            if intron is None: continue  # invalid intron format
            intron_chrom, intron_start, intron_stop, intron_cluster_id = intron
            # Get additional cluster signif info,
            # where cluster_signif look up
            # first_key = {chr}:{clust_id}
            # and second_key is each element in
            # PARSE_CLUSTER_SIGNIF
            _cluster_signif_values = get_additional_annotation_information(
                cluster_signif_dict,
                "{0}:{1}".format(intron_chrom, intron_cluster_id),
                PARSE_CLUSTER_SIGNIF
            )
            # Check if cluster meets FDR threshold
            try: fdr = float(_cluster_signif_values[ADJ_P_COLUMN_IDX])
            except ValueError: continue                # value cannot be type cast, i.e NA
            if fdr > float(args.fdr_filter): continue  # does not meet filter
            # Get additional intron info, where 
            # intron_ann first_key:
            # {chr}:{intron_start}:{intron_end}:{clust_id}
            # and second_key is each element in
            # PARSE_INTRON_ANN
            _intron_ann_values = get_additional_annotation_information(
                intron_ann_dict,
                ":".join(intron),
                PARSE_INTRON_ANN
            )
            # Get exon information for the
            # splice junction, where:
            #   key = "transcript:exonA_end:exonB_start"
            #   i.e "{transcript}:{intron_start}:{intron_end}"
            # and values are the features
            # in SINGULAR_EXON_FEATURES and
            # SURRONDING_EXON_FEATURES
            _exon_splice_junction_keys = [
                "{0}:{1}:{2}".format(t,intron_start,intron_stop) \
                for t in _intron_ann_values[INTRON_TRANSCRIPT_IDX].split("+")
            ]
            # Aggregate 1:M features across for
            # splice junctions effecting more than
            # one transcript
            _exon_features_dict = {}
            # Loop through each splice junction
            for sj in _exon_splice_junction_keys:
                for feature in SINGULAR_EXON_FEATURES + SURRONDING_EXON_FEATURES:
                    if feature not in _exon_features_dict:
                        _exon_features_dict[feature] = []
                    _exon_features_dict[feature].append(
                        splicing_ann_dict.get(sj, {}).get(feature, "NA")
                    )
            # Collapse exon-exon strand information,
            # it will be on the same strand for a
            # given transcript splice junction
            _exon_features_dict["exon_strand"] = ["|".join(_exon_features_dict["exon_strand"])]
            # Aggregate the exon information
            _exon_ann_values = []
            for feature in SINGULAR_EXON_FEATURES + SURRONDING_EXON_FEATURES:
                # Join multi valued features with the
                # multi_value_delim character
                _exon_ann_values.append(
                    "+".join(_exon_features_dict[feature])
                )
            # Write annotated line to output
            _output_line = "{0}\t{1}\t{2}\t{3}".format(
                "\t".join(tokens),
                "\t".join(_cluster_signif_values),
                "\t".join(_intron_ann_values),
                "\t".join(_exon_ann_values)
            )
            ofh.write(_output_line + "\n")
    log("Finished writing annotated output file: ", args.output)
